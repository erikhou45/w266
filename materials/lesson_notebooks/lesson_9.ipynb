{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Session 9\n",
    "\n",
    "**Note:** This notebook is to be seen as a 'scratch-pad', illustrating the points made in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation of Some Synthetic Test Data \n",
    "\n",
    "Imagine 400 train and 400 test examples with 10 features each. Each example has **two** labels, label 'a' and label 'b'. We construct these examples synthetically by picking 10 random numbers between 0 and 1, and adding for the examples with label $a =  1$ 0.2 to the first five features, and subtracting 0.2 in those features for examples with label $a = 0$. We do the same for label $b$, where we add/subtract 0.1 to features 6-10. (Obviously, this is just a toy example and the details do not matter - we just want to have some data to explore the formalisms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "train_X = np.concatenate([train_1_X, train_2_X, train_3_X, train_4_X])\n",
    "\n",
    "test_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "test_X = np.concatenate([test_1_X, test_2_X, test_3_X, test_4_X])\n",
    "\n",
    "train_1a_y = train_1b_y = train_2a_y = train_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "train_2b_y = train_3a_y = train_4a_y = train_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "test_1a_y = test_1b_y = test_2a_y = test_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "test_2b_y = test_3a_y = test_4a_y = test_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "train_a_y = np.concatenate([train_1a_y, train_2a_y, train_3a_y, train_4a_y])\n",
    "train_b_y = np.concatenate([train_1b_y, train_2b_y, train_3b_y, train_4b_y])\n",
    "\n",
    "test_a_y = np.concatenate([test_1a_y, test_2a_y, test_3a_y, test_4a_y])\n",
    "test_b_y = np.concatenate([test_1b_y, test_2b_y, test_3b_y, test_4b_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20090649, 0.5368248 , 0.75112714, 0.37645579, 1.01529157,\n",
       "        0.38071727, 0.60069459, 0.96073541, 1.06558354, 1.04067819],\n",
       "       [0.96083078, 0.44315406, 0.70021697, 0.47023007, 0.8673389 ,\n",
       "        0.9461193 , 0.25646862, 1.05648187, 0.34222064, 0.87750498]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_a_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_y[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.\n",
    "\n",
    "### 2. Keras' Functional API Formalism\n",
    "\n",
    "Keras' Functional API allows us to construct non-sequential models with multiple inputs, multiple outputs, branched models,  etc..\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* start with definition of inputs through Input layer(s)\n",
    "* define the layers and outputs as:\n",
    "$$\\rm next\\_layer\\_activations = layer(layer\\_parameters) (previous\\_layer\\_activations)$$\n",
    "* define the model the the input(s) and output(s).  \n",
    "* 'compile' the model by specifying loss function(s), metric(s) and the optimizer.  \n",
    "\n",
    "We first set things up for one binary classification. It is most convenient to define a function called *build_model()* where we can parametrize aspects we want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mhbutler/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mhbutler/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_numbers (InputLayer)   [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(activation_function='relu', \n",
    "                optimizer='adam', \n",
    "                kernel_initializer=tf.keras.initializers.he_normal(seed=1), \n",
    "                bias_initializer='zeros'):\n",
    "\n",
    "    # Define Input layer(s)\n",
    "    input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense') # layer definition\n",
    "    dense_output = dense_layer(input_numbers)  # layer acting on previous layer's output\n",
    "    \n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer_2 = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense_2') # layer definition\n",
    "    dense_output_2 = dense_layer_2(dense_output)  # layer acting on previous layer's output\n",
    "\n",
    "    \n",
    "\n",
    "    # Define classification layer and act on previous output to classify examples\n",
    "    classification_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='classification') # layer definition\n",
    "    classification_output = classification_layer(dense_output_2)   # layer acting on previous layer's output\n",
    "\n",
    "    # Build and compile model\n",
    "    model = tf.keras.models.Model(input_numbers, classification_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!   \n",
    "\n",
    "**Question:** Do the dimensions look right? \n",
    "\n",
    "### 3.  Training Optimizations\n",
    "\n",
    "Great, let's do some experimenting. We run the model above for the synthetic data set we defined, only considering the first label, and look at a few variations. Specifically, let's play with:\n",
    "\n",
    "* the **activation function** in the hidden layer     \n",
    "* the **initialization**    \n",
    "* the **optimizer**\n",
    "\n",
    "We run everything for 40 epochs and compare the results. (We split the training into two parts to cut down on reporting. 'verbose=0' supresses output. But we still want to see the final loss so we add one epoch with 'verbose=1'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 60us/sample - loss: 0.2285 - val_loss: 0.1856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef1827c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = build_model()  # This one should be good...\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our supposed best setup, and it defines our baselines. \n",
    "\n",
    "Let's veer off and see how loss, etc. change. First, we use $tanh()$ as opposed to $relu()$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 61us/sample - loss: 0.2601 - val_loss: 0.2040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef211a110>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = build_model(activation_function='tanh')  \n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse. [Of course, the initialization is random, so things may be different on future runs.]\n",
    "\n",
    "Next, we change the optimizer to stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 62us/sample - loss: 0.3979 - val_loss: 0.3742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef0095e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_model(optimizer='sgd')  \n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot worse!!\n",
    "\n",
    "Going back to the activation function, what about $sigmoid()$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 62us/sample - loss: 0.4683 - val_loss: 0.4538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef003f6d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = build_model(activation_function='sigmoid')  \n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good!\n",
    "\n",
    "Lastly, what about choosing general random matrix initialization as opposed to He/Xavier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mhbutler/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 63us/sample - loss: 0.1937 - val_loss: 0.1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ee971eed0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = build_model(kernel_initializer='random_normal')  \n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse too.\n",
    "\n",
    "**Critical Note:** We were cheating a bit here! There is a stochastic component and if one re-reruns the cells results will differ. In fact, *in this simple case* the last configuration is often the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting Models - Multiple Outputs\n",
    "\n",
    "Now, let's go back to our dataset and let's try to predict two labels. We want to model that by *splitting* the model, i.e., two distinct hidden layers act on the input, and we use those two branches to predict the two labels:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = tf.keras.layers.Dense(10, activation='relu', name='dense_1')\n",
    "dense_layer_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)     # hideen layer 1 acting on input\n",
    "dense_output_2 = dense_layer_2(input_numbers)     # hideen layer 2 acting on input\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')  # classification layer 1 for first branch\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')  # classification layer 2 for second branch\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_numbers (InputLayer)      [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "classification_1 (Dense)        (None, 1)            11          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification_2 (Dense)        (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 242\n",
      "Trainable params: 242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "Epoch 1/150\n",
      "400/400 [==============================] - 0s 1ms/sample - loss: 1.5079 - classification_1_loss: 0.7331 - classification_2_loss: 0.7740 - classification_1_acc: 0.4375 - classification_2_acc: 0.4925 - val_loss: 1.4894 - val_classification_1_loss: 0.7307 - val_classification_2_loss: 0.7637 - val_classification_1_acc: 0.4275 - val_classification_2_acc: 0.4825\n",
      "Epoch 2/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 1.4619 - classification_1_loss: 0.7168 - classification_2_loss: 0.7457 - classification_1_acc: 0.4525 - classification_2_acc: 0.4900 - val_loss: 1.4480 - val_classification_1_loss: 0.7147 - val_classification_2_loss: 0.7297 - val_classification_1_acc: 0.4500 - val_classification_2_acc: 0.4925\n",
      "Epoch 3/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.4241 - classification_1_loss: 0.7034 - classification_2_loss: 0.7202 - classification_1_acc: 0.4875 - classification_2_acc: 0.4925 - val_loss: 1.4149 - val_classification_1_loss: 0.7022 - val_classification_2_loss: 0.7115 - val_classification_1_acc: 0.4900 - val_classification_2_acc: 0.4875\n",
      "Epoch 4/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.3942 - classification_1_loss: 0.6943 - classification_2_loss: 0.6997 - classification_1_acc: 0.5100 - classification_2_acc: 0.4950 - val_loss: 1.3899 - val_classification_1_loss: 0.6928 - val_classification_2_loss: 0.6974 - val_classification_1_acc: 0.5450 - val_classification_2_acc: 0.5025\n",
      "Epoch 5/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 1.3699 - classification_1_loss: 0.6848 - classification_2_loss: 0.6849 - classification_1_acc: 0.5475 - classification_2_acc: 0.5525 - val_loss: 1.3710 - val_classification_1_loss: 0.6845 - val_classification_2_loss: 0.6860 - val_classification_1_acc: 0.5875 - val_classification_2_acc: 0.5625\n",
      "Epoch 6/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.3523 - classification_1_loss: 0.6755 - classification_2_loss: 0.6725 - classification_1_acc: 0.6200 - classification_2_acc: 0.5675 - val_loss: 1.3558 - val_classification_1_loss: 0.6748 - val_classification_2_loss: 0.6806 - val_classification_1_acc: 0.6325 - val_classification_2_acc: 0.5625\n",
      "Epoch 7/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 1.3377 - classification_1_loss: 0.6680 - classification_2_loss: 0.6714 - classification_1_acc: 0.6575 - classification_2_acc: 0.5925 - val_loss: 1.3448 - val_classification_1_loss: 0.6684 - val_classification_2_loss: 0.6750 - val_classification_1_acc: 0.6725 - val_classification_2_acc: 0.5700\n",
      "Epoch 8/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.3259 - classification_1_loss: 0.6609 - classification_2_loss: 0.6690 - classification_1_acc: 0.6875 - classification_2_acc: 0.5950 - val_loss: 1.3349 - val_classification_1_loss: 0.6613 - val_classification_2_loss: 0.6739 - val_classification_1_acc: 0.6925 - val_classification_2_acc: 0.5700\n",
      "Epoch 9/150\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 1.3147 - classification_1_loss: 0.6534 - classification_2_loss: 0.6610 - classification_1_acc: 0.7225 - classification_2_acc: 0.5975 - val_loss: 1.3248 - val_classification_1_loss: 0.6541 - val_classification_2_loss: 0.6717 - val_classification_1_acc: 0.7175 - val_classification_2_acc: 0.5725\n",
      "Epoch 10/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 1.3045 - classification_1_loss: 0.6465 - classification_2_loss: 0.6556 - classification_1_acc: 0.7525 - classification_2_acc: 0.5950 - val_loss: 1.3151 - val_classification_1_loss: 0.6465 - val_classification_2_loss: 0.6668 - val_classification_1_acc: 0.7425 - val_classification_2_acc: 0.5850\n",
      "Epoch 11/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 1.2929 - classification_1_loss: 0.6417 - classification_2_loss: 0.6542 - classification_1_acc: 0.7600 - classification_2_acc: 0.6025 - val_loss: 1.3042 - val_classification_1_loss: 0.6379 - val_classification_2_loss: 0.6605 - val_classification_1_acc: 0.7625 - val_classification_2_acc: 0.6025\n",
      "Epoch 12/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 1.2813 - classification_1_loss: 0.6333 - classification_2_loss: 0.6497 - classification_1_acc: 0.7825 - classification_2_acc: 0.6075 - val_loss: 1.2931 - val_classification_1_loss: 0.6308 - val_classification_2_loss: 0.6608 - val_classification_1_acc: 0.7775 - val_classification_2_acc: 0.6050\n",
      "Epoch 13/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.2688 - classification_1_loss: 0.6251 - classification_2_loss: 0.6449 - classification_1_acc: 0.7875 - classification_2_acc: 0.6175 - val_loss: 1.2814 - val_classification_1_loss: 0.6232 - val_classification_2_loss: 0.6571 - val_classification_1_acc: 0.7800 - val_classification_2_acc: 0.6175\n",
      "Epoch 14/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 1.2564 - classification_1_loss: 0.6157 - classification_2_loss: 0.6392 - classification_1_acc: 0.8025 - classification_2_acc: 0.6300 - val_loss: 1.2682 - val_classification_1_loss: 0.6156 - val_classification_2_loss: 0.6537 - val_classification_1_acc: 0.7925 - val_classification_2_acc: 0.6125\n",
      "Epoch 15/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 1.2425 - classification_1_loss: 0.6069 - classification_2_loss: 0.6340 - classification_1_acc: 0.8250 - classification_2_acc: 0.6375 - val_loss: 1.2558 - val_classification_1_loss: 0.6049 - val_classification_2_loss: 0.6534 - val_classification_1_acc: 0.8050 - val_classification_2_acc: 0.6200\n",
      "Epoch 16/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 1.2284 - classification_1_loss: 0.5978 - classification_2_loss: 0.6277 - classification_1_acc: 0.8250 - classification_2_acc: 0.6475 - val_loss: 1.2419 - val_classification_1_loss: 0.5948 - val_classification_2_loss: 0.6478 - val_classification_1_acc: 0.8250 - val_classification_2_acc: 0.6125\n",
      "Epoch 17/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 1.2144 - classification_1_loss: 0.5899 - classification_2_loss: 0.6261 - classification_1_acc: 0.8275 - classification_2_acc: 0.6525 - val_loss: 1.2279 - val_classification_1_loss: 0.5839 - val_classification_2_loss: 0.6458 - val_classification_1_acc: 0.8400 - val_classification_2_acc: 0.6150\n",
      "Epoch 18/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 1.1990 - classification_1_loss: 0.5783 - classification_2_loss: 0.6202 - classification_1_acc: 0.8350 - classification_2_acc: 0.6550 - val_loss: 1.2132 - val_classification_1_loss: 0.5725 - val_classification_2_loss: 0.6371 - val_classification_1_acc: 0.8450 - val_classification_2_acc: 0.6275\n",
      "Epoch 19/150\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 1.1836 - classification_1_loss: 0.5680 - classification_2_loss: 0.6166 - classification_1_acc: 0.8425 - classification_2_acc: 0.6650 - val_loss: 1.1989 - val_classification_1_loss: 0.5614 - val_classification_2_loss: 0.6343 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6350\n",
      "Epoch 20/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 1.1688 - classification_1_loss: 0.5582 - classification_2_loss: 0.6130 - classification_1_acc: 0.8475 - classification_2_acc: 0.6625 - val_loss: 1.1840 - val_classification_1_loss: 0.5504 - val_classification_2_loss: 0.6347 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6375\n",
      "Epoch 21/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 1.1531 - classification_1_loss: 0.5457 - classification_2_loss: 0.6053 - classification_1_acc: 0.8575 - classification_2_acc: 0.6725 - val_loss: 1.1699 - val_classification_1_loss: 0.5431 - val_classification_2_loss: 0.6302 - val_classification_1_acc: 0.8625 - val_classification_2_acc: 0.6400\n",
      "Epoch 22/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 1.1376 - classification_1_loss: 0.5360 - classification_2_loss: 0.6003 - classification_1_acc: 0.8700 - classification_2_acc: 0.6800 - val_loss: 1.1556 - val_classification_1_loss: 0.5310 - val_classification_2_loss: 0.6254 - val_classification_1_acc: 0.8675 - val_classification_2_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 1.1222 - classification_1_loss: 0.5245 - classification_2_loss: 0.5966 - classification_1_acc: 0.8725 - classification_2_acc: 0.6875 - val_loss: 1.1407 - val_classification_1_loss: 0.5202 - val_classification_2_loss: 0.6212 - val_classification_1_acc: 0.8725 - val_classification_2_acc: 0.6525\n",
      "Epoch 24/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 1.1066 - classification_1_loss: 0.5126 - classification_2_loss: 0.5885 - classification_1_acc: 0.8725 - classification_2_acc: 0.6850 - val_loss: 1.1259 - val_classification_1_loss: 0.5066 - val_classification_2_loss: 0.6160 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6575\n",
      "Epoch 25/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 1.0913 - classification_1_loss: 0.5087 - classification_2_loss: 0.5897 - classification_1_acc: 0.8700 - classification_2_acc: 0.6825 - val_loss: 1.1115 - val_classification_1_loss: 0.4969 - val_classification_2_loss: 0.6158 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6625\n",
      "Epoch 26/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 1.0768 - classification_1_loss: 0.4948 - classification_2_loss: 0.5836 - classification_1_acc: 0.8750 - classification_2_acc: 0.6925 - val_loss: 1.0971 - val_classification_1_loss: 0.4837 - val_classification_2_loss: 0.6098 - val_classification_1_acc: 0.8750 - val_classification_2_acc: 0.6675\n",
      "Epoch 27/150\n",
      "400/400 [==============================] - 0s 403us/sample - loss: 1.0618 - classification_1_loss: 0.4848 - classification_2_loss: 0.5814 - classification_1_acc: 0.8750 - classification_2_acc: 0.6900 - val_loss: 1.0835 - val_classification_1_loss: 0.4763 - val_classification_2_loss: 0.6053 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.6650\n",
      "Epoch 28/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 1.0468 - classification_1_loss: 0.4740 - classification_2_loss: 0.5734 - classification_1_acc: 0.8775 - classification_2_acc: 0.6900 - val_loss: 1.0696 - val_classification_1_loss: 0.4617 - val_classification_2_loss: 0.6113 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6750\n",
      "Epoch 29/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 1.0320 - classification_1_loss: 0.4634 - classification_2_loss: 0.5704 - classification_1_acc: 0.8775 - classification_2_acc: 0.7025 - val_loss: 1.0554 - val_classification_1_loss: 0.4571 - val_classification_2_loss: 0.5993 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6875\n",
      "Epoch 30/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 1.0179 - classification_1_loss: 0.4504 - classification_2_loss: 0.5614 - classification_1_acc: 0.8750 - classification_2_acc: 0.7050 - val_loss: 1.0419 - val_classification_1_loss: 0.4454 - val_classification_2_loss: 0.5989 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6975\n",
      "Epoch 31/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 1.0041 - classification_1_loss: 0.4417 - classification_2_loss: 0.5619 - classification_1_acc: 0.8725 - classification_2_acc: 0.7075 - val_loss: 1.0287 - val_classification_1_loss: 0.4344 - val_classification_2_loss: 0.5943 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7025\n",
      "Epoch 32/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.9905 - classification_1_loss: 0.4364 - classification_2_loss: 0.5541 - classification_1_acc: 0.8775 - classification_2_acc: 0.7125 - val_loss: 1.0158 - val_classification_1_loss: 0.4258 - val_classification_2_loss: 0.5877 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7000\n",
      "Epoch 33/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.9773 - classification_1_loss: 0.4291 - classification_2_loss: 0.5521 - classification_1_acc: 0.8800 - classification_2_acc: 0.7175 - val_loss: 1.0034 - val_classification_1_loss: 0.4146 - val_classification_2_loss: 0.5883 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7050\n",
      "Epoch 34/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.9643 - classification_1_loss: 0.4165 - classification_2_loss: 0.5470 - classification_1_acc: 0.8775 - classification_2_acc: 0.7200 - val_loss: 0.9917 - val_classification_1_loss: 0.4067 - val_classification_2_loss: 0.5891 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7100\n",
      "Epoch 35/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.9524 - classification_1_loss: 0.4060 - classification_2_loss: 0.5449 - classification_1_acc: 0.8750 - classification_2_acc: 0.7250 - val_loss: 0.9798 - val_classification_1_loss: 0.3970 - val_classification_2_loss: 0.5775 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7125\n",
      "Epoch 36/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.9408 - classification_1_loss: 0.3994 - classification_2_loss: 0.5352 - classification_1_acc: 0.8725 - classification_2_acc: 0.7325 - val_loss: 0.9683 - val_classification_1_loss: 0.3893 - val_classification_2_loss: 0.5820 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.7175\n",
      "Epoch 37/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.9289 - classification_1_loss: 0.3961 - classification_2_loss: 0.5366 - classification_1_acc: 0.8725 - classification_2_acc: 0.7375 - val_loss: 0.9575 - val_classification_1_loss: 0.3769 - val_classification_2_loss: 0.5741 - val_classification_1_acc: 0.8850 - val_classification_2_acc: 0.7200\n",
      "Epoch 38/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.9180 - classification_1_loss: 0.3817 - classification_2_loss: 0.5330 - classification_1_acc: 0.8750 - classification_2_acc: 0.7400 - val_loss: 0.9477 - val_classification_1_loss: 0.3715 - val_classification_2_loss: 0.5794 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7175\n",
      "Epoch 39/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.9079 - classification_1_loss: 0.3788 - classification_2_loss: 0.5290 - classification_1_acc: 0.8775 - classification_2_acc: 0.7275 - val_loss: 0.9377 - val_classification_1_loss: 0.3653 - val_classification_2_loss: 0.5770 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7200\n",
      "Epoch 40/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.8973 - classification_1_loss: 0.3703 - classification_2_loss: 0.5266 - classification_1_acc: 0.8775 - classification_2_acc: 0.7325 - val_loss: 0.9275 - val_classification_1_loss: 0.3568 - val_classification_2_loss: 0.5741 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7200\n",
      "Epoch 41/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.8868 - classification_1_loss: 0.3623 - classification_2_loss: 0.5197 - classification_1_acc: 0.8800 - classification_2_acc: 0.7425 - val_loss: 0.9182 - val_classification_1_loss: 0.3493 - val_classification_2_loss: 0.5732 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7225\n",
      "Epoch 42/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.8782 - classification_1_loss: 0.3605 - classification_2_loss: 0.5190 - classification_1_acc: 0.8800 - classification_2_acc: 0.7400 - val_loss: 0.9092 - val_classification_1_loss: 0.3448 - val_classification_2_loss: 0.5639 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7225\n",
      "Epoch 43/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.8683 - classification_1_loss: 0.3490 - classification_2_loss: 0.5167 - classification_1_acc: 0.8775 - classification_2_acc: 0.7525 - val_loss: 0.9002 - val_classification_1_loss: 0.3374 - val_classification_2_loss: 0.5637 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7200\n",
      "Epoch 44/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.8596 - classification_1_loss: 0.3440 - classification_2_loss: 0.5142 - classification_1_acc: 0.8775 - classification_2_acc: 0.7550 - val_loss: 0.8916 - val_classification_1_loss: 0.3334 - val_classification_2_loss: 0.5610 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.8508 - classification_1_loss: 0.3383 - classification_2_loss: 0.5096 - classification_1_acc: 0.8775 - classification_2_acc: 0.7500 - val_loss: 0.8834 - val_classification_1_loss: 0.3204 - val_classification_2_loss: 0.5586 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7225\n",
      "Epoch 46/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.8424 - classification_1_loss: 0.3345 - classification_2_loss: 0.5052 - classification_1_acc: 0.8800 - classification_2_acc: 0.7475 - val_loss: 0.8761 - val_classification_1_loss: 0.3219 - val_classification_2_loss: 0.5539 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7225\n",
      "Epoch 47/150\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 0.8344 - classification_1_loss: 0.3281 - classification_2_loss: 0.5031 - classification_1_acc: 0.8800 - classification_2_acc: 0.7475 - val_loss: 0.8679 - val_classification_1_loss: 0.3195 - val_classification_2_loss: 0.5521 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7225\n",
      "Epoch 48/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.8266 - classification_1_loss: 0.3247 - classification_2_loss: 0.5027 - classification_1_acc: 0.8800 - classification_2_acc: 0.7500 - val_loss: 0.8606 - val_classification_1_loss: 0.3072 - val_classification_2_loss: 0.5539 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7275\n",
      "Epoch 49/150\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 0.8198 - classification_1_loss: 0.3145 - classification_2_loss: 0.4966 - classification_1_acc: 0.8825 - classification_2_acc: 0.7475 - val_loss: 0.8538 - val_classification_1_loss: 0.3045 - val_classification_2_loss: 0.5513 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7225\n",
      "Epoch 50/150\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 0.8132 - classification_1_loss: 0.3149 - classification_2_loss: 0.4964 - classification_1_acc: 0.8825 - classification_2_acc: 0.7450 - val_loss: 0.8469 - val_classification_1_loss: 0.2942 - val_classification_2_loss: 0.5541 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7275\n",
      "Epoch 51/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.8051 - classification_1_loss: 0.3148 - classification_2_loss: 0.4986 - classification_1_acc: 0.8850 - classification_2_acc: 0.7625 - val_loss: 0.8401 - val_classification_1_loss: 0.2921 - val_classification_2_loss: 0.5495 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7275\n",
      "Epoch 52/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.7993 - classification_1_loss: 0.3095 - classification_2_loss: 0.4933 - classification_1_acc: 0.8875 - classification_2_acc: 0.7575 - val_loss: 0.8336 - val_classification_1_loss: 0.2866 - val_classification_2_loss: 0.5402 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7300\n",
      "Epoch 53/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.7918 - classification_1_loss: 0.3047 - classification_2_loss: 0.4936 - classification_1_acc: 0.8925 - classification_2_acc: 0.7525 - val_loss: 0.8271 - val_classification_1_loss: 0.2848 - val_classification_2_loss: 0.5456 - val_classification_1_acc: 0.9275 - val_classification_2_acc: 0.7300\n",
      "Epoch 54/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.7858 - classification_1_loss: 0.3030 - classification_2_loss: 0.4887 - classification_1_acc: 0.8925 - classification_2_acc: 0.7525 - val_loss: 0.8211 - val_classification_1_loss: 0.2786 - val_classification_2_loss: 0.5425 - val_classification_1_acc: 0.9275 - val_classification_2_acc: 0.7300\n",
      "Epoch 55/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.7804 - classification_1_loss: 0.2933 - classification_2_loss: 0.4897 - classification_1_acc: 0.8900 - classification_2_acc: 0.7625 - val_loss: 0.8149 - val_classification_1_loss: 0.2744 - val_classification_2_loss: 0.5426 - val_classification_1_acc: 0.9275 - val_classification_2_acc: 0.7325\n",
      "Epoch 56/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.7743 - classification_1_loss: 0.2942 - classification_2_loss: 0.4816 - classification_1_acc: 0.8925 - classification_2_acc: 0.7600 - val_loss: 0.8092 - val_classification_1_loss: 0.2683 - val_classification_2_loss: 0.5455 - val_classification_1_acc: 0.9250 - val_classification_2_acc: 0.7300\n",
      "Epoch 57/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.7688 - classification_1_loss: 0.2873 - classification_2_loss: 0.4845 - classification_1_acc: 0.8925 - classification_2_acc: 0.7600 - val_loss: 0.8040 - val_classification_1_loss: 0.2647 - val_classification_2_loss: 0.5380 - val_classification_1_acc: 0.9325 - val_classification_2_acc: 0.7275\n",
      "Epoch 58/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.7631 - classification_1_loss: 0.2859 - classification_2_loss: 0.4764 - classification_1_acc: 0.8925 - classification_2_acc: 0.7600 - val_loss: 0.7985 - val_classification_1_loss: 0.2664 - val_classification_2_loss: 0.5389 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7275\n",
      "Epoch 59/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.7574 - classification_1_loss: 0.2795 - classification_2_loss: 0.4759 - classification_1_acc: 0.8925 - classification_2_acc: 0.7700 - val_loss: 0.7928 - val_classification_1_loss: 0.2582 - val_classification_2_loss: 0.5361 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7275\n",
      "Epoch 60/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.7522 - classification_1_loss: 0.2779 - classification_2_loss: 0.4754 - classification_1_acc: 0.8925 - classification_2_acc: 0.7650 - val_loss: 0.7874 - val_classification_1_loss: 0.2544 - val_classification_2_loss: 0.5335 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7300\n",
      "Epoch 61/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.7477 - classification_1_loss: 0.2710 - classification_2_loss: 0.4692 - classification_1_acc: 0.8925 - classification_2_acc: 0.7675 - val_loss: 0.7824 - val_classification_1_loss: 0.2550 - val_classification_2_loss: 0.5261 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7400\n",
      "Epoch 62/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.7429 - classification_1_loss: 0.2734 - classification_2_loss: 0.4682 - classification_1_acc: 0.8925 - classification_2_acc: 0.7725 - val_loss: 0.7774 - val_classification_1_loss: 0.2557 - val_classification_2_loss: 0.5248 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7350\n",
      "Epoch 63/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.7374 - classification_1_loss: 0.2674 - classification_2_loss: 0.4716 - classification_1_acc: 0.8950 - classification_2_acc: 0.7775 - val_loss: 0.7728 - val_classification_1_loss: 0.2500 - val_classification_2_loss: 0.5253 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7350\n",
      "Epoch 64/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.7326 - classification_1_loss: 0.2667 - classification_2_loss: 0.4668 - classification_1_acc: 0.8950 - classification_2_acc: 0.7725 - val_loss: 0.7681 - val_classification_1_loss: 0.2445 - val_classification_2_loss: 0.5260 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7425\n",
      "Epoch 65/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.7277 - classification_1_loss: 0.2604 - classification_2_loss: 0.4644 - classification_1_acc: 0.8975 - classification_2_acc: 0.7850 - val_loss: 0.7635 - val_classification_1_loss: 0.2370 - val_classification_2_loss: 0.5276 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7375\n",
      "Epoch 66/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.7233 - classification_1_loss: 0.2620 - classification_2_loss: 0.4639 - classification_1_acc: 0.8950 - classification_2_acc: 0.7800 - val_loss: 0.7593 - val_classification_1_loss: 0.2361 - val_classification_2_loss: 0.5257 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.7198 - classification_1_loss: 0.2585 - classification_2_loss: 0.4605 - classification_1_acc: 0.8950 - classification_2_acc: 0.7800 - val_loss: 0.7551 - val_classification_1_loss: 0.2365 - val_classification_2_loss: 0.5251 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7400\n",
      "Epoch 68/150\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.7148 - classification_1_loss: 0.2647 - classification_2_loss: 0.4562 - classification_1_acc: 0.8950 - classification_2_acc: 0.7800 - val_loss: 0.7506 - val_classification_1_loss: 0.2319 - val_classification_2_loss: 0.5171 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7450\n",
      "Epoch 69/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.7120 - classification_1_loss: 0.2603 - classification_2_loss: 0.4569 - classification_1_acc: 0.9000 - classification_2_acc: 0.7800 - val_loss: 0.7468 - val_classification_1_loss: 0.2257 - val_classification_2_loss: 0.5202 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7425\n",
      "Epoch 70/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.7065 - classification_1_loss: 0.2494 - classification_2_loss: 0.4547 - classification_1_acc: 0.8950 - classification_2_acc: 0.7850 - val_loss: 0.7434 - val_classification_1_loss: 0.2291 - val_classification_2_loss: 0.5150 - val_classification_1_acc: 0.9400 - val_classification_2_acc: 0.7500\n",
      "Epoch 71/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.7032 - classification_1_loss: 0.2504 - classification_2_loss: 0.4578 - classification_1_acc: 0.8950 - classification_2_acc: 0.7900 - val_loss: 0.7399 - val_classification_1_loss: 0.2216 - val_classification_2_loss: 0.5155 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7575\n",
      "Epoch 72/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6998 - classification_1_loss: 0.2506 - classification_2_loss: 0.4571 - classification_1_acc: 0.8950 - classification_2_acc: 0.7950 - val_loss: 0.7354 - val_classification_1_loss: 0.2184 - val_classification_2_loss: 0.5205 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7450\n",
      "Epoch 73/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6953 - classification_1_loss: 0.2506 - classification_2_loss: 0.4566 - classification_1_acc: 0.9000 - classification_2_acc: 0.7900 - val_loss: 0.7318 - val_classification_1_loss: 0.2174 - val_classification_2_loss: 0.5163 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7450\n",
      "Epoch 74/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6918 - classification_1_loss: 0.2482 - classification_2_loss: 0.4467 - classification_1_acc: 0.9000 - classification_2_acc: 0.7925 - val_loss: 0.7284 - val_classification_1_loss: 0.2147 - val_classification_2_loss: 0.5098 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7525\n",
      "Epoch 75/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.6879 - classification_1_loss: 0.2378 - classification_2_loss: 0.4496 - classification_1_acc: 0.8975 - classification_2_acc: 0.7950 - val_loss: 0.7250 - val_classification_1_loss: 0.2174 - val_classification_2_loss: 0.5101 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7500\n",
      "Epoch 76/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6851 - classification_1_loss: 0.2446 - classification_2_loss: 0.4446 - classification_1_acc: 0.8975 - classification_2_acc: 0.8000 - val_loss: 0.7216 - val_classification_1_loss: 0.2104 - val_classification_2_loss: 0.5189 - val_classification_1_acc: 0.9400 - val_classification_2_acc: 0.7475\n",
      "Epoch 77/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6816 - classification_1_loss: 0.2397 - classification_2_loss: 0.4504 - classification_1_acc: 0.9025 - classification_2_acc: 0.8000 - val_loss: 0.7187 - val_classification_1_loss: 0.2125 - val_classification_2_loss: 0.5105 - val_classification_1_acc: 0.9350 - val_classification_2_acc: 0.7500\n",
      "Epoch 78/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6788 - classification_1_loss: 0.2336 - classification_2_loss: 0.4430 - classification_1_acc: 0.9075 - classification_2_acc: 0.7950 - val_loss: 0.7150 - val_classification_1_loss: 0.2092 - val_classification_2_loss: 0.5017 - val_classification_1_acc: 0.9325 - val_classification_2_acc: 0.7500\n",
      "Epoch 79/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.6750 - classification_1_loss: 0.2319 - classification_2_loss: 0.4411 - classification_1_acc: 0.9050 - classification_2_acc: 0.8050 - val_loss: 0.7114 - val_classification_1_loss: 0.2039 - val_classification_2_loss: 0.4987 - val_classification_1_acc: 0.9400 - val_classification_2_acc: 0.7475\n",
      "Epoch 80/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6727 - classification_1_loss: 0.2362 - classification_2_loss: 0.4342 - classification_1_acc: 0.9000 - classification_2_acc: 0.8025 - val_loss: 0.7090 - val_classification_1_loss: 0.2014 - val_classification_2_loss: 0.5065 - val_classification_1_acc: 0.9425 - val_classification_2_acc: 0.7450\n",
      "Epoch 81/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6695 - classification_1_loss: 0.2286 - classification_2_loss: 0.4386 - classification_1_acc: 0.9050 - classification_2_acc: 0.8075 - val_loss: 0.7057 - val_classification_1_loss: 0.2018 - val_classification_2_loss: 0.5033 - val_classification_1_acc: 0.9425 - val_classification_2_acc: 0.7475\n",
      "Epoch 82/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6681 - classification_1_loss: 0.2241 - classification_2_loss: 0.4379 - classification_1_acc: 0.9100 - classification_2_acc: 0.8000 - val_loss: 0.7040 - val_classification_1_loss: 0.2052 - val_classification_2_loss: 0.4975 - val_classification_1_acc: 0.9450 - val_classification_2_acc: 0.7550\n",
      "Epoch 83/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.6638 - classification_1_loss: 0.2228 - classification_2_loss: 0.4377 - classification_1_acc: 0.9100 - classification_2_acc: 0.7950 - val_loss: 0.7012 - val_classification_1_loss: 0.1956 - val_classification_2_loss: 0.5035 - val_classification_1_acc: 0.9450 - val_classification_2_acc: 0.7525\n",
      "Epoch 84/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6623 - classification_1_loss: 0.2247 - classification_2_loss: 0.4455 - classification_1_acc: 0.9125 - classification_2_acc: 0.8100 - val_loss: 0.6973 - val_classification_1_loss: 0.1981 - val_classification_2_loss: 0.4933 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7475\n",
      "Epoch 85/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6594 - classification_1_loss: 0.2247 - classification_2_loss: 0.4298 - classification_1_acc: 0.9125 - classification_2_acc: 0.7950 - val_loss: 0.6956 - val_classification_1_loss: 0.1930 - val_classification_2_loss: 0.4980 - val_classification_1_acc: 0.9375 - val_classification_2_acc: 0.7550\n",
      "Epoch 86/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6559 - classification_1_loss: 0.2220 - classification_2_loss: 0.4359 - classification_1_acc: 0.9125 - classification_2_acc: 0.8025 - val_loss: 0.6937 - val_classification_1_loss: 0.1962 - val_classification_2_loss: 0.5052 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7525\n",
      "Epoch 87/150\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.6535 - classification_1_loss: 0.2239 - classification_2_loss: 0.4380 - classification_1_acc: 0.9075 - classification_2_acc: 0.8025 - val_loss: 0.6900 - val_classification_1_loss: 0.1920 - val_classification_2_loss: 0.4956 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7500\n",
      "Epoch 88/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6504 - classification_1_loss: 0.2144 - classification_2_loss: 0.4374 - classification_1_acc: 0.9100 - classification_2_acc: 0.8050 - val_loss: 0.6882 - val_classification_1_loss: 0.1888 - val_classification_2_loss: 0.5007 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6481 - classification_1_loss: 0.2146 - classification_2_loss: 0.4246 - classification_1_acc: 0.9150 - classification_2_acc: 0.8025 - val_loss: 0.6858 - val_classification_1_loss: 0.1887 - val_classification_2_loss: 0.4910 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7575\n",
      "Epoch 90/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6465 - classification_1_loss: 0.2121 - classification_2_loss: 0.4312 - classification_1_acc: 0.9125 - classification_2_acc: 0.8050 - val_loss: 0.6834 - val_classification_1_loss: 0.1894 - val_classification_2_loss: 0.4992 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7550\n",
      "Epoch 91/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6444 - classification_1_loss: 0.2121 - classification_2_loss: 0.4306 - classification_1_acc: 0.9150 - classification_2_acc: 0.7975 - val_loss: 0.6801 - val_classification_1_loss: 0.1830 - val_classification_2_loss: 0.4864 - val_classification_1_acc: 0.9425 - val_classification_2_acc: 0.7575\n",
      "Epoch 92/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6410 - classification_1_loss: 0.2161 - classification_2_loss: 0.4313 - classification_1_acc: 0.9175 - classification_2_acc: 0.8075 - val_loss: 0.6790 - val_classification_1_loss: 0.1815 - val_classification_2_loss: 0.5001 - val_classification_1_acc: 0.9425 - val_classification_2_acc: 0.7600\n",
      "Epoch 93/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6394 - classification_1_loss: 0.2119 - classification_2_loss: 0.4292 - classification_1_acc: 0.9150 - classification_2_acc: 0.8100 - val_loss: 0.6769 - val_classification_1_loss: 0.1807 - val_classification_2_loss: 0.4945 - val_classification_1_acc: 0.9450 - val_classification_2_acc: 0.7600\n",
      "Epoch 94/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6366 - classification_1_loss: 0.2112 - classification_2_loss: 0.4305 - classification_1_acc: 0.9150 - classification_2_acc: 0.8075 - val_loss: 0.6741 - val_classification_1_loss: 0.1793 - val_classification_2_loss: 0.4913 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7600\n",
      "Epoch 95/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6355 - classification_1_loss: 0.2129 - classification_2_loss: 0.4312 - classification_1_acc: 0.9175 - classification_2_acc: 0.8000 - val_loss: 0.6719 - val_classification_1_loss: 0.1783 - val_classification_2_loss: 0.4850 - val_classification_1_acc: 0.9450 - val_classification_2_acc: 0.7575\n",
      "Epoch 96/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6329 - classification_1_loss: 0.2153 - classification_2_loss: 0.4279 - classification_1_acc: 0.9225 - classification_2_acc: 0.8075 - val_loss: 0.6715 - val_classification_1_loss: 0.1753 - val_classification_2_loss: 0.4985 - val_classification_1_acc: 0.9425 - val_classification_2_acc: 0.7625\n",
      "Epoch 97/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6314 - classification_1_loss: 0.2078 - classification_2_loss: 0.4283 - classification_1_acc: 0.9200 - classification_2_acc: 0.8050 - val_loss: 0.6696 - val_classification_1_loss: 0.1787 - val_classification_2_loss: 0.4867 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7625\n",
      "Epoch 98/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.6298 - classification_1_loss: 0.2085 - classification_2_loss: 0.4236 - classification_1_acc: 0.9200 - classification_2_acc: 0.8150 - val_loss: 0.6663 - val_classification_1_loss: 0.1763 - val_classification_2_loss: 0.4827 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7625\n",
      "Epoch 99/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.6274 - classification_1_loss: 0.2054 - classification_2_loss: 0.4229 - classification_1_acc: 0.9175 - classification_2_acc: 0.8100 - val_loss: 0.6635 - val_classification_1_loss: 0.1717 - val_classification_2_loss: 0.4841 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7625\n",
      "Epoch 100/150\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.6255 - classification_1_loss: 0.2040 - classification_2_loss: 0.4233 - classification_1_acc: 0.9200 - classification_2_acc: 0.8150 - val_loss: 0.6620 - val_classification_1_loss: 0.1727 - val_classification_2_loss: 0.4870 - val_classification_1_acc: 0.9450 - val_classification_2_acc: 0.7575\n",
      "Epoch 101/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6231 - classification_1_loss: 0.2028 - classification_2_loss: 0.4147 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6608 - val_classification_1_loss: 0.1719 - val_classification_2_loss: 0.4810 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7650\n",
      "Epoch 102/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.6218 - classification_1_loss: 0.2058 - classification_2_loss: 0.4244 - classification_1_acc: 0.9225 - classification_2_acc: 0.8075 - val_loss: 0.6593 - val_classification_1_loss: 0.1689 - val_classification_2_loss: 0.4926 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7650\n",
      "Epoch 103/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6202 - classification_1_loss: 0.1978 - classification_2_loss: 0.4241 - classification_1_acc: 0.9225 - classification_2_acc: 0.8125 - val_loss: 0.6580 - val_classification_1_loss: 0.1712 - val_classification_2_loss: 0.4903 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7600\n",
      "Epoch 104/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6180 - classification_1_loss: 0.1979 - classification_2_loss: 0.4156 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6552 - val_classification_1_loss: 0.1707 - val_classification_2_loss: 0.4936 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7600\n",
      "Epoch 105/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6167 - classification_1_loss: 0.1944 - classification_2_loss: 0.4160 - classification_1_acc: 0.9225 - classification_2_acc: 0.8125 - val_loss: 0.6537 - val_classification_1_loss: 0.1675 - val_classification_2_loss: 0.4838 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7625\n",
      "Epoch 106/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.6158 - classification_1_loss: 0.1992 - classification_2_loss: 0.4215 - classification_1_acc: 0.9200 - classification_2_acc: 0.8175 - val_loss: 0.6521 - val_classification_1_loss: 0.1689 - val_classification_2_loss: 0.4902 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7600\n",
      "Epoch 107/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6132 - classification_1_loss: 0.1979 - classification_2_loss: 0.4104 - classification_1_acc: 0.9225 - classification_2_acc: 0.8125 - val_loss: 0.6511 - val_classification_1_loss: 0.1659 - val_classification_2_loss: 0.4785 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7650\n",
      "Epoch 108/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6126 - classification_1_loss: 0.1991 - classification_2_loss: 0.4184 - classification_1_acc: 0.9225 - classification_2_acc: 0.8125 - val_loss: 0.6504 - val_classification_1_loss: 0.1648 - val_classification_2_loss: 0.4832 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7650\n",
      "Epoch 109/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6106 - classification_1_loss: 0.2029 - classification_2_loss: 0.4149 - classification_1_acc: 0.9200 - classification_2_acc: 0.8100 - val_loss: 0.6481 - val_classification_1_loss: 0.1638 - val_classification_2_loss: 0.4808 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7625\n",
      "Epoch 110/150\n",
      "400/400 [==============================] - 0s 97us/sample - loss: 0.6090 - classification_1_loss: 0.1951 - classification_2_loss: 0.4122 - classification_1_acc: 0.9225 - classification_2_acc: 0.8150 - val_loss: 0.6472 - val_classification_1_loss: 0.1634 - val_classification_2_loss: 0.4857 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6072 - classification_1_loss: 0.1923 - classification_2_loss: 0.4111 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6462 - val_classification_1_loss: 0.1631 - val_classification_2_loss: 0.4909 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7650\n",
      "Epoch 112/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.6063 - classification_1_loss: 0.1898 - classification_2_loss: 0.4073 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6452 - val_classification_1_loss: 0.1598 - val_classification_2_loss: 0.4882 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7650\n",
      "Epoch 113/150\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.6046 - classification_1_loss: 0.1958 - classification_2_loss: 0.4087 - classification_1_acc: 0.9225 - classification_2_acc: 0.8125 - val_loss: 0.6431 - val_classification_1_loss: 0.1612 - val_classification_2_loss: 0.4810 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7600\n",
      "Epoch 114/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.6040 - classification_1_loss: 0.1907 - classification_2_loss: 0.4089 - classification_1_acc: 0.9225 - classification_2_acc: 0.8150 - val_loss: 0.6420 - val_classification_1_loss: 0.1572 - val_classification_2_loss: 0.4816 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7650\n",
      "Epoch 115/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6031 - classification_1_loss: 0.1884 - classification_2_loss: 0.4091 - classification_1_acc: 0.9225 - classification_2_acc: 0.8175 - val_loss: 0.6402 - val_classification_1_loss: 0.1591 - val_classification_2_loss: 0.4813 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7650\n",
      "Epoch 116/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.6009 - classification_1_loss: 0.1939 - classification_2_loss: 0.4116 - classification_1_acc: 0.9225 - classification_2_acc: 0.8175 - val_loss: 0.6399 - val_classification_1_loss: 0.1561 - val_classification_2_loss: 0.4797 - val_classification_1_acc: 0.9475 - val_classification_2_acc: 0.7650\n",
      "Epoch 117/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.5995 - classification_1_loss: 0.1926 - classification_2_loss: 0.4062 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6393 - val_classification_1_loss: 0.1591 - val_classification_2_loss: 0.4908 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7675\n",
      "Epoch 118/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.5989 - classification_1_loss: 0.1929 - classification_2_loss: 0.4065 - classification_1_acc: 0.9200 - classification_2_acc: 0.8100 - val_loss: 0.6385 - val_classification_1_loss: 0.1570 - val_classification_2_loss: 0.4737 - val_classification_1_acc: 0.9600 - val_classification_2_acc: 0.7625\n",
      "Epoch 119/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.5986 - classification_1_loss: 0.1898 - classification_2_loss: 0.4061 - classification_1_acc: 0.9225 - classification_2_acc: 0.8075 - val_loss: 0.6374 - val_classification_1_loss: 0.1583 - val_classification_2_loss: 0.4761 - val_classification_1_acc: 0.9600 - val_classification_2_acc: 0.7650\n",
      "Epoch 120/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.5957 - classification_1_loss: 0.1875 - classification_2_loss: 0.4048 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6352 - val_classification_1_loss: 0.1553 - val_classification_2_loss: 0.4749 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7625\n",
      "Epoch 121/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5948 - classification_1_loss: 0.1856 - classification_2_loss: 0.4057 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6343 - val_classification_1_loss: 0.1520 - val_classification_2_loss: 0.4771 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7625\n",
      "Epoch 122/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.5940 - classification_1_loss: 0.1883 - classification_2_loss: 0.4012 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6333 - val_classification_1_loss: 0.1523 - val_classification_2_loss: 0.4884 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7625\n",
      "Epoch 123/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.5932 - classification_1_loss: 0.1851 - classification_2_loss: 0.4059 - classification_1_acc: 0.9250 - classification_2_acc: 0.8100 - val_loss: 0.6328 - val_classification_1_loss: 0.1543 - val_classification_2_loss: 0.4796 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7650\n",
      "Epoch 124/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5924 - classification_1_loss: 0.1846 - classification_2_loss: 0.4001 - classification_1_acc: 0.9225 - classification_2_acc: 0.8100 - val_loss: 0.6326 - val_classification_1_loss: 0.1509 - val_classification_2_loss: 0.4796 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7700\n",
      "Epoch 125/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.5917 - classification_1_loss: 0.1902 - classification_2_loss: 0.4031 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6302 - val_classification_1_loss: 0.1537 - val_classification_2_loss: 0.4725 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7600\n",
      "Epoch 126/150\n",
      "400/400 [==============================] - 0s 91us/sample - loss: 0.5901 - classification_1_loss: 0.1853 - classification_2_loss: 0.4158 - classification_1_acc: 0.9275 - classification_2_acc: 0.8200 - val_loss: 0.6293 - val_classification_1_loss: 0.1521 - val_classification_2_loss: 0.4670 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7650\n",
      "Epoch 127/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5892 - classification_1_loss: 0.1829 - classification_2_loss: 0.4099 - classification_1_acc: 0.9250 - classification_2_acc: 0.8100 - val_loss: 0.6294 - val_classification_1_loss: 0.1497 - val_classification_2_loss: 0.4787 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7700\n",
      "Epoch 128/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.5875 - classification_1_loss: 0.1865 - classification_2_loss: 0.3995 - classification_1_acc: 0.9250 - classification_2_acc: 0.8150 - val_loss: 0.6285 - val_classification_1_loss: 0.1514 - val_classification_2_loss: 0.4806 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7700\n",
      "Epoch 129/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5865 - classification_1_loss: 0.1847 - classification_2_loss: 0.4044 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6277 - val_classification_1_loss: 0.1510 - val_classification_2_loss: 0.4786 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7700\n",
      "Epoch 130/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.5855 - classification_1_loss: 0.1812 - classification_2_loss: 0.4036 - classification_1_acc: 0.9275 - classification_2_acc: 0.8225 - val_loss: 0.6267 - val_classification_1_loss: 0.1480 - val_classification_2_loss: 0.4698 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7625\n",
      "Epoch 131/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.5844 - classification_1_loss: 0.1829 - classification_2_loss: 0.3975 - classification_1_acc: 0.9250 - classification_2_acc: 0.8225 - val_loss: 0.6254 - val_classification_1_loss: 0.1487 - val_classification_2_loss: 0.4690 - val_classification_1_acc: 0.9500 - val_classification_2_acc: 0.7700\n",
      "Epoch 132/150\n",
      "400/400 [==============================] - 0s 98us/sample - loss: 0.5835 - classification_1_loss: 0.1821 - classification_2_loss: 0.4015 - classification_1_acc: 0.9250 - classification_2_acc: 0.8150 - val_loss: 0.6252 - val_classification_1_loss: 0.1458 - val_classification_2_loss: 0.4740 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/150\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.5834 - classification_1_loss: 0.1788 - classification_2_loss: 0.3994 - classification_1_acc: 0.9275 - classification_2_acc: 0.8175 - val_loss: 0.6247 - val_classification_1_loss: 0.1485 - val_classification_2_loss: 0.4849 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7675\n",
      "Epoch 134/150\n",
      "400/400 [==============================] - 0s 98us/sample - loss: 0.5825 - classification_1_loss: 0.1789 - classification_2_loss: 0.3969 - classification_1_acc: 0.9250 - classification_2_acc: 0.8150 - val_loss: 0.6235 - val_classification_1_loss: 0.1490 - val_classification_2_loss: 0.4683 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7700\n",
      "Epoch 135/150\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.5816 - classification_1_loss: 0.1829 - classification_2_loss: 0.3995 - classification_1_acc: 0.9250 - classification_2_acc: 0.8200 - val_loss: 0.6228 - val_classification_1_loss: 0.1441 - val_classification_2_loss: 0.4763 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7700\n",
      "Epoch 136/150\n",
      "400/400 [==============================] - 0s 97us/sample - loss: 0.5802 - classification_1_loss: 0.1774 - classification_2_loss: 0.4035 - classification_1_acc: 0.9275 - classification_2_acc: 0.8150 - val_loss: 0.6227 - val_classification_1_loss: 0.1447 - val_classification_2_loss: 0.4716 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7700\n",
      "Epoch 137/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.5801 - classification_1_loss: 0.1792 - classification_2_loss: 0.4013 - classification_1_acc: 0.9275 - classification_2_acc: 0.8150 - val_loss: 0.6204 - val_classification_1_loss: 0.1472 - val_classification_2_loss: 0.4737 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7650\n",
      "Epoch 138/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5791 - classification_1_loss: 0.1761 - classification_2_loss: 0.4008 - classification_1_acc: 0.9275 - classification_2_acc: 0.8200 - val_loss: 0.6208 - val_classification_1_loss: 0.1435 - val_classification_2_loss: 0.4764 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7700\n",
      "Epoch 139/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5778 - classification_1_loss: 0.1780 - classification_2_loss: 0.3960 - classification_1_acc: 0.9300 - classification_2_acc: 0.8175 - val_loss: 0.6194 - val_classification_1_loss: 0.1432 - val_classification_2_loss: 0.4732 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7700\n",
      "Epoch 140/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5769 - classification_1_loss: 0.1763 - classification_2_loss: 0.4052 - classification_1_acc: 0.9300 - classification_2_acc: 0.8175 - val_loss: 0.6187 - val_classification_1_loss: 0.1467 - val_classification_2_loss: 0.4724 - val_classification_1_acc: 0.9525 - val_classification_2_acc: 0.7700\n",
      "Epoch 141/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5765 - classification_1_loss: 0.1778 - classification_2_loss: 0.4016 - classification_1_acc: 0.9275 - classification_2_acc: 0.8150 - val_loss: 0.6187 - val_classification_1_loss: 0.1412 - val_classification_2_loss: 0.4683 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7700\n",
      "Epoch 142/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.5762 - classification_1_loss: 0.1763 - classification_2_loss: 0.4059 - classification_1_acc: 0.9300 - classification_2_acc: 0.8150 - val_loss: 0.6170 - val_classification_1_loss: 0.1439 - val_classification_2_loss: 0.4725 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7700\n",
      "Epoch 143/150\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.5747 - classification_1_loss: 0.1763 - classification_2_loss: 0.3986 - classification_1_acc: 0.9300 - classification_2_acc: 0.8150 - val_loss: 0.6174 - val_classification_1_loss: 0.1437 - val_classification_2_loss: 0.4717 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7675\n",
      "Epoch 144/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5748 - classification_1_loss: 0.1740 - classification_2_loss: 0.3901 - classification_1_acc: 0.9275 - classification_2_acc: 0.8150 - val_loss: 0.6167 - val_classification_1_loss: 0.1456 - val_classification_2_loss: 0.4745 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7675\n",
      "Epoch 145/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5775 - classification_1_loss: 0.1761 - classification_2_loss: 0.3936 - classification_1_acc: 0.9275 - classification_2_acc: 0.8150 - val_loss: 0.6142 - val_classification_1_loss: 0.1482 - val_classification_2_loss: 0.4760 - val_classification_1_acc: 0.9600 - val_classification_2_acc: 0.7675\n",
      "Epoch 146/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5732 - classification_1_loss: 0.1819 - classification_2_loss: 0.3936 - classification_1_acc: 0.9275 - classification_2_acc: 0.8175 - val_loss: 0.6151 - val_classification_1_loss: 0.1456 - val_classification_2_loss: 0.4741 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7675\n",
      "Epoch 147/150\n",
      "400/400 [==============================] - 0s 93us/sample - loss: 0.5722 - classification_1_loss: 0.1804 - classification_2_loss: 0.3936 - classification_1_acc: 0.9300 - classification_2_acc: 0.8150 - val_loss: 0.6154 - val_classification_1_loss: 0.1495 - val_classification_2_loss: 0.4709 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7675\n",
      "Epoch 148/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5716 - classification_1_loss: 0.1724 - classification_2_loss: 0.4002 - classification_1_acc: 0.9300 - classification_2_acc: 0.8125 - val_loss: 0.6149 - val_classification_1_loss: 0.1391 - val_classification_2_loss: 0.4757 - val_classification_1_acc: 0.9550 - val_classification_2_acc: 0.7675\n",
      "Epoch 149/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.5722 - classification_1_loss: 0.1707 - classification_2_loss: 0.3984 - classification_1_acc: 0.9300 - classification_2_acc: 0.8150 - val_loss: 0.6135 - val_classification_1_loss: 0.1390 - val_classification_2_loss: 0.4769 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7700\n",
      "Epoch 150/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.5713 - classification_1_loss: 0.1747 - classification_2_loss: 0.4049 - classification_1_acc: 0.9300 - classification_2_acc: 0.8100 - val_loss: 0.6143 - val_classification_1_loss: 0.1384 - val_classification_2_loss: 0.4725 - val_classification_1_acc: 0.9575 - val_classification_2_acc: 0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ee95cc650>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=150,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "* Does it surprise you that the loss for label 1 is smaller than the loss for label 2?    \n",
    "* Quick side check... do we overtrain?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Layers\n",
    "\n",
    "Let's define a simple custom layer which is nothing but an affine transformation followed by a relu activation. We need to define:\n",
    "* \\_\\_init\\_\\_()      \n",
    "* build()   \n",
    "* call()   \n",
    "* compute_output_shape()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MyStupidDenseLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):   \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super(MyStupidDenseLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_variable(\"W\",\n",
    "                                    shape=[int(input_shape[1]),self.output_dim], \n",
    "                                   initializer=tf.glorot_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b = self.add_variable(\"b\",\n",
    "                                    shape=[self.output_dim, ], \n",
    "                                   initializer=tf.initializers.zeros(),\n",
    "                                   trainable=True)\n",
    "        super(MyStupidDenseLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.nn.relu(tf.tensordot(x, self.W, axes=[[1],[0]]) + self.b)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our custom layer in the same architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-f08987fc569e>:16: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    }
   ],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.5857 - classification_1_loss: 0.1766 - classification_2_loss: 0.4084 - classification_1_acc: 0.9325 - classification_2_acc: 0.7950 - val_loss: 0.6058 - val_classification_1_loss: 0.1326 - val_classification_2_loss: 0.4780 - val_classification_1_acc: 0.9625 - val_classification_2_acc: 0.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ef21ca690>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=149,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. So the custom layer works exactly as expected.\n",
    "\n",
    "\n",
    "### 6. Sharing & Freezing Weights \n",
    "\n",
    "\n",
    "Let's define an identical model. The weights for this model will be re-initialized, so upon training for 1 epoch the loss will be much higher than after 150 epochs for the original model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 1ms/sample - loss: 1.4511 - classification_1_loss: 0.7201 - classification_2_loss: 0.7294 - classification_1_acc: 0.4925 - classification_2_acc: 0.4475 - val_loss: 1.4177 - val_classification_1_loss: 0.7029 - val_classification_2_loss: 0.7156 - val_classification_1_acc: 0.4950 - val_classification_2_acc: 0.4425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ee90cbed0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this model would make very different predictions - the loss is ~1.4 vs ~0.6. No surprise, obviously, as this model has not been trained.\n",
    "\n",
    "What if we manually set the the weights of the second model to the weights of the first? Let's look at the layers for both models and then copy the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f9ef21caa90>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f9ef21ca990>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f9ef21ba850>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9ee95bbe10>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9ee9750890>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one layer in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would that be? No magic.. weights and biases of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. What is the name of the layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct. Is it trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Are the layers of the second model essentially the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f9ee9269710>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f9ee92696d0>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f9ee9269750>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9ee9269dd0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9ee9269d50>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup. (We could look at shapes, etc, but won't do that now.) Now we set the second model's weights to equal the first model's trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why don't we set the weights for layer 0?\n",
    "\n",
    "Okay. What is the loss now for the mirror model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 99us/sample - loss: 0.5866 - classification_1_loss: 0.1706 - classification_2_loss: 0.4157 - classification_1_acc: 0.9300 - classification_2_acc: 0.7900 - val_loss: 0.6071 - val_classification_1_loss: 0.1360 - val_classification_2_loss: 0.4744 - val_classification_1_acc: 0.9625 - val_classification_2_acc: 0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f68153250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Essentially the same as for the original one! (Not a surprise actually... but great that it works.)\n",
    "\n",
    "Let's now turn to **freezing** layers. For example, you may want to copy weights from another model but hold those weights fixed upon further training. To test this, we will do a simple toy exercise: freeze a layer of the second model and train it further. Let's compare some weights before and after training. \n",
    "\n",
    "We start by defining a new model that again has the same architecture, **but we set the trainable-parameter for one layer to False**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "dense_layer_2.trainable = False                                   # Freeze this layer\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror_2 = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror_2.compile(loss=losses,  optimizer='adam', metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again set the weights to mirror the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror_2.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror_2.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror_2.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror_2.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are (some of) the weights now, before we train further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5493507 ,  0.64608824, -0.4960204 ],\n",
       "       [ 0.5602562 ,  0.54325235, -0.7760406 ],\n",
       "       [ 0.33898988,  0.8414829 ,  0.03868393]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03614025, -0.07353805,  0.03746113],\n",
       "       [-0.06931327, -0.41153473, -0.25313395],\n",
       "       [-0.4576991 , -0.01952373,  0.2951088 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare that with the values post further training. Let's also check the 'trainable' settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so it shows that one hidden layer supposedly is trainable, the other one is not. Is that true? We train more and compare weights compared to what they were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 97us/sample - loss: 0.5713 - classification_1_loss: 0.1696 - classification_2_loss: 0.4112 - classification_1_acc: 0.9350 - classification_2_acc: 0.7950 - val_loss: 0.5972 - val_classification_1_loss: 0.1281 - val_classification_2_loss: 0.4683 - val_classification_1_acc: 0.9725 - val_classification_2_acc: 0.7625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ee8e98f90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=40,\n",
    "    verbose=0\n",
    ")\n",
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5515234 ,  0.648011  , -0.5317562 ],\n",
       "       [ 0.5594    ,  0.5445362 , -0.80385584],\n",
       "       [ 0.34686086,  0.8477436 ,  0.02895916]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03614025, -0.07353805,  0.03746113],\n",
       "       [-0.06931327, -0.41153473, -0.25313395],\n",
       "       [-0.4576991 , -0.01952373,  0.2951088 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did not change**. So here we could see layer freezing at work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
