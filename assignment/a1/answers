# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 1 parts for a total of 18 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Information Theory (18 points)



###################################################################
###################################################################
## Information Theory (18 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (Code): Binary Entropy (1 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is BinaryEntropy(0.67)?
information_theory_code_1: 0.91493


# ------------------------------------------------------------------
# | Section (A): Pointwise Mutual Information (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is PMI(rainy, cloudy)?
information_theory_a_1: 0.32193

# Question 2 (/1): What is PMI(washington, post)?
information_theory_a_2: 7.81378


# ------------------------------------------------------------------
# | Section (B): Entropy (5 points)  | 
# ------------------------------------------------------------------

# Question 1.1 (/1): Expected bits for uniform probability, 128 messages?
information_theory_b_1_1: 7

# Question 1.2 (/1): Entropy for uniform probability, 128 messages?
information_theory_b_1_2: 7

# Question 1.3 (/1): Entropy for uniform probability, 1024 messages?
information_theory_b_1_3: 10

# Question 2 (/1): Which has higher entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_b_2: 
 - B

# Question 3 (/1): Which has higher entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_b_3: 
 - A


# ------------------------------------------------------------------
# | Section (C): Cross-Entropy and KL Divergence (10 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the Cross Entropy?
information_theory_c_1: 0.51457

# Question 2 (/1): What is the KL divergence?
information_theory_c_2: 0.51457

# Question 3 (/5): Do you actually need to compute the everything?  (Hint... think of the value of most terms in the summation)
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_3: 
 - False

# Question 4 (/1): What if the model put all the mass on the correct class?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_4: 
 - Zero


# Question 5 (/1): What if the model put all the mass on class 1?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_5: 
 - Infinite


# Question 6 (/1): What if the model put 1/3 of the mass on classes 1, 2, 3?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_6: 
 - Infinite
