# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 46 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Language Models (17 points)
# - Language Models Notebook (16 points)
# - Window and Recurrent Models (13 points)



###################################################################
###################################################################
## Language Models (17 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Preliminaries (unrelated to a notebook) (17 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the goal of a language model?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_1_1: 
 - Translate between languages.
 - Score whether a sequence of tokens belongs to a language.
 - Model sentiment of a sentence.

# Question 2 (/1): If you don't have much training data, what might happen if you increase "n" of n-grams?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_1_2: 
 - Model performance will decrease despite increased sparsity
 - Model performance will improve despite increased sparsity

# Question 3 (/1): If you have lots of training data, what might happen if you increase "n" of n-grams?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_1_3: 
 - Model performance will decrease despite increased sparsity
 - Model performance will improve despite increased sparsity

# Question 4 (/2): A reasonable technique to handle out-of-vocabulary words?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_1_4: 
 - Replacing digits with a special token
 - Replacing rare words in the training set with a special UNK token to get estimates for what contexts these tend to appear in
 - All of the above

# Question 5 (/2): Smoothing is a technique to... (hint - remember P(next_word | context) must add up to 1 across all next_word-s.
# (This question is multiple choice.  Delete all but the correct answer).
language_models_1_5: 
 - Handle sparsity issues even while increasing the amount of context considered
 - Take the probability distribution of words that appear in a context in the training set and flatten / smooth it by taking probability from very high probability words and spreading it across the rest of the vocabulary
 - All of the above

# Question 6 (/5): Let C[x, y, z] be the number of times the text "x y z" appeared in the corpus.  C[the cat] = 8000, C[the cat in] = 5000, C[the cat in the] = 1000 and C[the cat in the hat] = 800.  The size of your vocabulary is 10,000.  What is P(hat | the cat in the)?
language_models_1_6: 0.00000

# Question 7 (/5): Let C[x, y, z] be the number of times the text "x y z" appeared in the corpus.  C[the cat] = 8000, C[the cat in] = 5000, C[the cat in the] = 1000 and C[the cat in the hat] = 800.  The size of your vocabulary is 10,000.  What is P(hat | the cat in the) if you use "AddK smoothing" with k=5?
language_models_1_7: 0.00000



###################################################################
###################################################################
## Language Models Notebook (16 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): AddK (ngram.ipynb) (8 points)  | 
# ------------------------------------------------------------------

# Question 1 (/3): How does your answer depend on k?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_a_1: 
 - Linearly
 - Inversely
 - Quadratically
 - It does not.

# Question 2 (/1): P(q | b, a)?
language_models_notebook_a_2: 0.00000

# Question 3 (/2): Which context scores the unknown word higher?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_a_3: 
 - a
 - b
 - same

# Question 4 (/2): Which should be higher?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_a_4: 
 - a
 - b
 - same


# ------------------------------------------------------------------
# | Section (e): Additional Questions (ngram.ipynb) (6 points)  | 
# ------------------------------------------------------------------

# Question 1a (/2): What is the average number of times we see a trigram that appears at least once?
language_models_notebook_e_1a: 0.00000

# Question 1b (/1): What is the average number of times we see all trigrams (i.e. including those that appeared zero times)?  Hint - look at the size of the vocabulary!
language_models_notebook_e_1b: 0.00000

# Question 2 (/2): Which would you expect would perform better?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_e_2: 
 - trigram
 - 5-gram

# Question 3 (/1): Which produces more realistic sentences?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_e_3: 
 - Simple Trigram
 - AddK


# ------------------------------------------------------------------
# | Section (f): Just For Fun (ngram.ipynb) (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What was the score of the highest scoring adjective sequence?
language_models_notebook_f_1: 0.00000

# Question 2 (/1): Did it match the meme?
# (This question is multiple choice.  Delete all but the correct answer).
language_models_notebook_f_2: 
 - True
 - False



###################################################################
###################################################################
## Window and Recurrent Models (13 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Neural network topology understanding (13 points)  | 
# ------------------------------------------------------------------

# Question 1 (/5): What is the main benefit to a window model over a RNN?
# (This question is multiple choice.  Delete all but the correct answer).
window_and_recurrent_models_a_1: 
 - Fast to train and use
 - Better capture context

# Question 2 (/5): What is the computational complexity of the final affine and softmax (over V classes) with hidden layer of dimension h?  Hint. a matrix multiplication of m x n by n x o is O(mno).  Assume each other mathematical operation is O(1).
# (This question is multiple choice.  Delete all but the correct answer).
window_and_recurrent_models_a_2: 
 - O(hV)
 - O(hV^2)
 - O(V)
 - Something else

# Question 3 (/1): In the RNN/LSTM slides (https://docs.google.com/presentation/d/1ZkbbbwtZY1PRgaBmRSAONMuiJaxaiaOQh5q14lhdyY8/edit#slide=id.g5b125d627b_10_12), when two wires merge in the diagrams, what does that mean?
# (This question is multiple choice.  Delete all but the correct answer).
window_and_recurrent_models_a_3: 
 - dot product
 - element-wise addition
 - element-wise subtraction
 - element-wise multiplication
 - concatenation

# Question 4 (/2): What are dangers of training RNNs (of any type) with long sequences?
# (This question is multiple choice.  Delete all but the correct answer).
window_and_recurrent_models_a_4: 
 - Exploding gradient
 - Vanishing gradient
 - Derivative too complex to compute
