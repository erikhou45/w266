{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "This notebook introduces convolutional neural networks (CNNs), a more powerful classification model similar to the Neural Bag-of-Words (BOW) model you explored earlier.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Part (a):** Model Architecture\n",
    "- **Part (b):** Implementing the CNN Model\n",
    "- **Part (c):** Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from w266_common import patched_numpy_io\n",
    "# Code for this assignment\n",
    "import sst\n",
    "\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Model Architecture\n",
    "\n",
    "CNNs are a more sophisticated neural model for sentence classification than the Neural BOW model we saw in the last section. CNNs operate by sweeping a collection of filters over a text. Each filter produces a sequence of feature values known as a _feature map_. In one of the most basic formulations introduced by [Kim (2014)](https://www.aclweb.org/anthology/D14-1181), a single layer of _pooling_ is used to summarize the _feature maps_ as a fixed length vector. The fixed length vector is then feed to the output layer in order to produce classification labels. A popular choice for the pooling operation is to take the maximum feature value from by each _feature map_.\n",
    "\n",
    "![Convolutional Neural Network from Kim 2014](kim_2014_figure_1_cnn.png)\n",
    "*CNN model architure, Figure 1 from Kim (2014)*\n",
    "\n",
    "We'll use the following notation:\n",
    "- $h$: filter/kernel length (in words)\n",
    "- $w^{(i)} \\in \\mathbb{Z}$, the word id for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x^{i:i+j}$ is the concatenation of $x^{(i)}, x^{(i+1)} ... x^{(i+j)}$ \n",
    "- $c^{(i)}_{k}$ is the value of the $k^{th}$ feature map at the $i^{th}$ position along the word sequence, each filter applies over a window of $h$ words and uses non-linearity $f$.\n",
    "- $c_k$ is one feature map (the $k_{th}$).  Its values are $c^{(0)}_{k}$, $c^{(1)}_{k}$, $c^{(2)}_{k}$, ...\n",
    "- $\\hat{c}_{k}$ is the value of the $k^{th}$ feature after pooling the feature map over the whole sequence.\n",
    "- $\\hat{C}$ is the concatenation of pooled feature maps. \n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Convolutional layer:** $c^{(i)}_{k} = f(x^{i:i+h-1} W_k + b)$\n",
    "- **Pooling layer:**  $\\hat{c}_{k}$ = $max(c_k)$ = $max(c^{(0)}_{k}, c^{(1)}_{k}...)$ \n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(\\hat{C} W_{out} + b_{out})$\n",
    "\n",
    "\n",
    "We'll refer to the first part of this model (**Embedding layer**, **Convolutional layer**, and **Pooling layer**) as the **Encoder**: it has the role of encoding the input sequence into a fixed-length vector representation that we pass to the output layer.\n",
    "\n",
    "We'll also use these as shorthand for important dimensions:\n",
    "- `V`: the vocabulary size (equal to `ds.vocab.size`)\n",
    "- `N`: the maximum number of tokens in the input text\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `filters`: number filters per filter length\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Short Answer Questions\n",
    "\n",
    "When answering these questions in the answers file,\n",
    "`embed_dim = 10`, `kernel_size = 3`, `filters=128`, `N=10` and `num_classes = 7`.  Assume a single example (no batching).\n",
    "\n",
    "1. In terms of these values, the vocabulary size `V` and the maximum sequence length `N`, what are the\n",
    "   shapes of the following variables: \n",
    "   $c^{(i)}_{k}$, $c_{k}$, $\\hat{c}_{k}$, and $\\hat{C}$. Assume a stride size of 1. Assume padding is not used (e.g., for tf.nn.max_pool and tf.nn.conv1d, setting padding='VALID'), provide the shapes listed above.\n",
    "<p>\n",
    "2. What are the shapes of $c_{k}$ and $\\hat{c}_{k}$ when padding is used.\n",
    "      (e.g., for tf.nn.max_pool and tf.nn.conv1d, setting padding='same').\n",
    "<p>\n",
    "3. How many parameters are in each of the convolutional filters, $W_{k}$?  What if the kernel size is 4? 5? And the output layer, $W_{out}$?\n",
    "<p>\n",
    "<p>\n",
    "4. Historically NLP models made heavy use of manual feature engineering. In relation to systems with manually engineered features, describe what type of operation is being performed by the convolutional filters.\n",
    "<p>\n",
    "5. Suppose that we have two examples, `[foo bar baz]` and `[baz bar foo]`.  Does this model definitely make the same predictions on these? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the CNN Model\n",
    "\n",
    "We'll implement our CNN model below. Our implementation will differ from [Kim (2014)](https://www.aclweb.org/anthology/D14-1181) in that we will support using multiple dense hidden layers after the convolutional layers.\n",
    "\n",
    "**Before you start**, be sure to answer the short-answer questions above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Building vocabulary - 16,474 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "\n",
    "# Load SST dataset\n",
    "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")\n",
    "max_len = 40\n",
    "train_x, train_ns, train_y = ds.as_padded_array('train', max_len=max_len, root_only=True)\n",
    "dev_x,   dev_ns,   dev_y   = ds.as_padded_array('dev',   max_len=max_len, root_only=True)\n",
    "test_x,  test_ns,  test_y  = ds.as_padded_array('test',  max_len=max_len, root_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 10\n",
    "embed_dim = 5\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = []\n",
    "dropout_rate = 0.8\n",
    "num_classes = len(ds.target_names)\n",
    "\n",
    "# Construct the convolutional neural network.\n",
    "# The form of each keras layer function is as follows:\n",
    "#    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "# concretely,\n",
    "#    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "# performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 10 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "h = keras.layers.Dense(10, activation='relu')(h)\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6920 samples\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 2s 273us/sample - loss: 0.6929 - acc: 0.5165\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 1s 168us/sample - loss: 0.6920 - acc: 0.5233\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 1s 165us/sample - loss: 0.6899 - acc: 0.5263\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 1s 180us/sample - loss: 0.6807 - acc: 0.5293\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 1s 197us/sample - loss: 0.6593 - acc: 0.5763\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 1s 200us/sample - loss: 0.6299 - acc: 0.6312\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 1s 202us/sample - loss: 0.6058 - acc: 0.6579\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 1s 215us/sample - loss: 0.5893 - acc: 0.6782\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 1s 198us/sample - loss: 0.5722 - acc: 0.6932\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 1s 210us/sample - loss: 0.5599 - acc: 0.7001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75f119bb90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "model.fit(train_x, train_y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Call [evaluate](https://keras.io/models/model/#evaluate) on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872/872 [==============================] - 1s 793us/sample - loss: 0.5894 - acc: 0.7053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5893976754004803, 0.70527524]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "model.evaluate(dev_x, dev_y)\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c): Tuning Your Model\n",
    "\n",
    "We'll once again want to optimize hyperparameters for our model to see if we can improve performance. The CNN model includes a number of new parameters that can significantly influence model performance.\n",
    "\n",
    "In this section, you will be asked to describe the new parameters as well as use them to attempt to improve the performance of your model.\n",
    "\n",
    "## Part (c) Short Answer Questions\n",
    "\n",
    "  1. Choose two parameters unique the CNN model, perform at least 10 runs with different combinations of values for these parameters, and then report the dev set results below. ***Hint: Consider wrapping the training code above in a for loop the examines the different values.***  To do this efficiently, you should consider [this paper](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) from Bergstra and Bengio.  [This blog post](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) also has a less formal treatment of the same topic.\n",
    "  2. Describe any trends you see in experiments above (e.g., can you identify good ranges for the individual parameters; are there any interesting interactions?)\n",
    "  3. Pick the three best configurations according to the dev set and evaluate them on the test data. Is the ranking of the three best models the same on the dev and test sets?\n",
    "  4. What was the best accuracy you achieved on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(num_filters=[2, 2, 2], kernel_sizes=[2, 3, 4]):\n",
    "    \n",
    "    embed_dim=5\n",
    "    dropout_rate = 0.8\n",
    "    num_classes = 2\n",
    "\n",
    "    # Construct the convolutional neural network.\n",
    "    # The form of each keras layer function is as follows:\n",
    "    #    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "    # concretely,\n",
    "    #    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "    # performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "    # Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "    # In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "#     print(num_filters)\n",
    "#     print(kernel_sizes)\n",
    "    wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "    # Embed the wordids.\n",
    "    # Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "    h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "    # Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "    # With the default hyperparameters, we construct 10 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "    # is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "    # function name below).\n",
    "    conv_layers_for_all_kernel_sizes = []\n",
    "    for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "    # Concat the feature maps from each different size.\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "    # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "    # in the vector.\n",
    "    # See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "    h = keras.layers.Dense(10, activation='relu')(h)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "    model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik_hou/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 426us/sample - loss: 0.6448 - acc: 0.6672\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 213us/sample - loss: 0.6218 - acc: 0.6885\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 221us/sample - loss: 0.6140 - acc: 0.6885\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 227us/sample - loss: 0.6084 - acc: 0.6885\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 257us/sample - loss: 0.5962 - acc: 0.6885\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 227us/sample - loss: 0.5753 - acc: 0.6887\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 215us/sample - loss: 0.5372 - acc: 0.6917\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 202us/sample - loss: 0.5035 - acc: 0.7117\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 235us/sample - loss: 0.4647 - acc: 0.7537\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 228us/sample - loss: 0.4330 - acc: 0.8077\n",
      "2307/2307 [==============================] - 1s 521us/sample - loss: 1.0690 - acc: 0.4573\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 452us/sample - loss: 0.6916 - acc: 0.5235\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 244us/sample - loss: 0.6861 - acc: 0.5378\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 209us/sample - loss: 0.6779 - acc: 0.5502\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 220us/sample - loss: 0.6552 - acc: 0.6150\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 229us/sample - loss: 0.6040 - acc: 0.7058\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 209us/sample - loss: 0.5423 - acc: 0.7524\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 217us/sample - loss: 0.4790 - acc: 0.7960\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 231us/sample - loss: 0.4116 - acc: 0.8329\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 227us/sample - loss: 0.3726 - acc: 0.8485\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 223us/sample - loss: 0.3356 - acc: 0.8715\n",
      "2307/2307 [==============================] - 1s 495us/sample - loss: 0.7133 - acc: 0.6879\n",
      "Train on 4614 samples\n",
      "Epoch 1/10\n",
      "4614/4614 [==============================] - 2s 435us/sample - loss: 0.6168 - acc: 0.7111\n",
      "Epoch 2/10\n",
      "4614/4614 [==============================] - 1s 221us/sample - loss: 0.5932 - acc: 0.7204\n",
      "Epoch 3/10\n",
      "4614/4614 [==============================] - 1s 234us/sample - loss: 0.5791 - acc: 0.7204\n",
      "Epoch 4/10\n",
      "4614/4614 [==============================] - 1s 236us/sample - loss: 0.5650 - acc: 0.7204\n",
      "Epoch 5/10\n",
      "4614/4614 [==============================] - 1s 211us/sample - loss: 0.5239 - acc: 0.7204\n",
      "Epoch 6/10\n",
      "4614/4614 [==============================] - 1s 224us/sample - loss: 0.4758 - acc: 0.7204\n",
      "Epoch 7/10\n",
      "4614/4614 [==============================] - 1s 237us/sample - loss: 0.4247 - acc: 0.7612\n",
      "Epoch 8/10\n",
      "4614/4614 [==============================] - 1s 236us/sample - loss: 0.3862 - acc: 0.8453\n",
      "Epoch 9/10\n",
      "4614/4614 [==============================] - 1s 267us/sample - loss: 0.3580 - acc: 0.8615\n",
      "Epoch 10/10\n",
      "4614/4614 [==============================] - 2s 341us/sample - loss: 0.3141 - acc: 0.8903\n",
      "2306/2306 [==============================] - 1s 534us/sample - loss: 1.4149 - acc: 0.5113\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 415us/sample - loss: 0.6441 - acc: 0.6813\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 199us/sample - loss: 0.6222 - acc: 0.6885\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 217us/sample - loss: 0.6144 - acc: 0.6885\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 195us/sample - loss: 0.6063 - acc: 0.6885\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 237us/sample - loss: 0.6014 - acc: 0.6885\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 232us/sample - loss: 0.5902 - acc: 0.6885\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 208us/sample - loss: 0.5829 - acc: 0.6885\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 199us/sample - loss: 0.5776 - acc: 0.6885\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 196us/sample - loss: 0.5722 - acc: 0.6885\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 234us/sample - loss: 0.5628 - acc: 0.6885\n",
      "2307/2307 [==============================] - 1s 524us/sample - loss: 0.9539 - acc: 0.0581\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 438us/sample - loss: 0.6917 - acc: 0.5266\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 222us/sample - loss: 0.6891 - acc: 0.5389\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 222us/sample - loss: 0.6854 - acc: 0.5458\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 201us/sample - loss: 0.6799 - acc: 0.5543\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 215us/sample - loss: 0.6739 - acc: 0.5573\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 204us/sample - loss: 0.6666 - acc: 0.5604\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 207us/sample - loss: 0.6545 - acc: 0.5686\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 204us/sample - loss: 0.6460 - acc: 0.5693\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 220us/sample - loss: 0.6434 - acc: 0.5727\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 206us/sample - loss: 0.6443 - acc: 0.5745\n",
      "2307/2307 [==============================] - 1s 544us/sample - loss: 0.6623 - acc: 0.6003\n",
      "Train on 4614 samples\n",
      "Epoch 1/10\n",
      "4614/4614 [==============================] - 2s 457us/sample - loss: 0.6343 - acc: 0.7005\n",
      "Epoch 2/10\n",
      "4614/4614 [==============================] - 1s 204us/sample - loss: 0.5948 - acc: 0.7204\n",
      "Epoch 3/10\n",
      "4614/4614 [==============================] - 1s 209us/sample - loss: 0.5884 - acc: 0.7206\n",
      "Epoch 4/10\n",
      "4614/4614 [==============================] - 1s 216us/sample - loss: 0.5809 - acc: 0.7219\n",
      "Epoch 5/10\n",
      "4614/4614 [==============================] - 1s 210us/sample - loss: 0.5758 - acc: 0.7271\n",
      "Epoch 6/10\n",
      "4614/4614 [==============================] - 1s 220us/sample - loss: 0.5649 - acc: 0.7421\n",
      "Epoch 7/10\n",
      "4614/4614 [==============================] - 1s 207us/sample - loss: 0.5689 - acc: 0.7373\n",
      "Epoch 8/10\n",
      "4614/4614 [==============================] - 1s 234us/sample - loss: 0.5528 - acc: 0.7514\n",
      "Epoch 9/10\n",
      "4614/4614 [==============================] - 1s 221us/sample - loss: 0.5456 - acc: 0.7564\n",
      "Epoch 10/10\n",
      "4614/4614 [==============================] - 1s 204us/sample - loss: 0.5372 - acc: 0.7592\n",
      "2306/2306 [==============================] - 1s 559us/sample - loss: 1.0522 - acc: 0.2003\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 466us/sample - loss: 0.6359 - acc: 0.6829\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 198us/sample - loss: 0.6192 - acc: 0.6885\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 208us/sample - loss: 0.6090 - acc: 0.6885\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 215us/sample - loss: 0.5973 - acc: 0.6885\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 228us/sample - loss: 0.5726 - acc: 0.6885\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 255us/sample - loss: 0.5483 - acc: 0.6885\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 244us/sample - loss: 0.5174 - acc: 0.6885\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 241us/sample - loss: 0.4825 - acc: 0.6956\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 233us/sample - loss: 0.4712 - acc: 0.7407\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 262us/sample - loss: 0.4452 - acc: 0.7605\n",
      "2307/2307 [==============================] - 1s 615us/sample - loss: 0.9365 - acc: 0.5098\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 505us/sample - loss: 0.6927 - acc: 0.5181\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 249us/sample - loss: 0.6897 - acc: 0.5363\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 200us/sample - loss: 0.6869 - acc: 0.5458\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4613/4613 [==============================] - 1s 236us/sample - loss: 0.6789 - acc: 0.5636\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 220us/sample - loss: 0.6688 - acc: 0.5732\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 218us/sample - loss: 0.6466 - acc: 0.6007\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 225us/sample - loss: 0.6073 - acc: 0.6432\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 236us/sample - loss: 0.5739 - acc: 0.6766\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 241us/sample - loss: 0.5398 - acc: 0.6974\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 237us/sample - loss: 0.5109 - acc: 0.7115\n",
      "2307/2307 [==============================] - 1s 537us/sample - loss: 0.5727 - acc: 0.7195\n",
      "Train on 4614 samples\n",
      "Epoch 1/10\n",
      "4614/4614 [==============================] - 2s 480us/sample - loss: 0.6206 - acc: 0.7156\n",
      "Epoch 2/10\n",
      "4614/4614 [==============================] - 1s 243us/sample - loss: 0.5974 - acc: 0.7204\n",
      "Epoch 3/10\n",
      "4614/4614 [==============================] - 1s 225us/sample - loss: 0.5885 - acc: 0.7204\n",
      "Epoch 4/10\n",
      "4614/4614 [==============================] - 1s 229us/sample - loss: 0.5819 - acc: 0.7204\n",
      "Epoch 5/10\n",
      "4614/4614 [==============================] - 1s 254us/sample - loss: 0.5683 - acc: 0.7204\n",
      "Epoch 6/10\n",
      "4614/4614 [==============================] - 1s 242us/sample - loss: 0.5548 - acc: 0.7204\n",
      "Epoch 7/10\n",
      "4614/4614 [==============================] - 1s 243us/sample - loss: 0.5336 - acc: 0.7204\n",
      "Epoch 8/10\n",
      "4614/4614 [==============================] - 1s 249us/sample - loss: 0.5156 - acc: 0.7204\n",
      "Epoch 9/10\n",
      "4614/4614 [==============================] - 1s 251us/sample - loss: 0.4891 - acc: 0.7204\n",
      "Epoch 10/10\n",
      "4614/4614 [==============================] - 1s 241us/sample - loss: 0.4694 - acc: 0.7319\n",
      "2306/2306 [==============================] - 1s 608us/sample - loss: 1.0312 - acc: 0.1370\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 498us/sample - loss: 0.6434 - acc: 0.6781\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 252us/sample - loss: 0.6195 - acc: 0.6885\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 255us/sample - loss: 0.6100 - acc: 0.6885\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 221us/sample - loss: 0.5959 - acc: 0.6885\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 216us/sample - loss: 0.5728 - acc: 0.6885\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 250us/sample - loss: 0.5440 - acc: 0.6887\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 223us/sample - loss: 0.5051 - acc: 0.7002\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 222us/sample - loss: 0.4664 - acc: 0.7529\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 235us/sample - loss: 0.4267 - acc: 0.7932\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 251us/sample - loss: 0.3994 - acc: 0.8071\n",
      "2307/2307 [==============================] - 1s 595us/sample - loss: 0.9600 - acc: 0.5674\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 510us/sample - loss: 0.6927 - acc: 0.5142\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 222us/sample - loss: 0.6896 - acc: 0.5376\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 235us/sample - loss: 0.6860 - acc: 0.5504\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 239us/sample - loss: 0.6687 - acc: 0.6089\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 256us/sample - loss: 0.6293 - acc: 0.6677\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 227us/sample - loss: 0.5821 - acc: 0.7086\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 237us/sample - loss: 0.5309 - acc: 0.7377\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 219us/sample - loss: 0.4752 - acc: 0.7778\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 215us/sample - loss: 0.4245 - acc: 0.7984\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 221us/sample - loss: 0.4000 - acc: 0.8138\n",
      "2307/2307 [==============================] - 1s 621us/sample - loss: 0.5559 - acc: 0.7217\n",
      "Train on 4614 samples\n",
      "Epoch 1/10\n",
      "4614/4614 [==============================] - 3s 562us/sample - loss: 0.6332 - acc: 0.7039\n",
      "Epoch 2/10\n",
      "4614/4614 [==============================] - 1s 256us/sample - loss: 0.5982 - acc: 0.7204\n",
      "Epoch 3/10\n",
      "4614/4614 [==============================] - 1s 269us/sample - loss: 0.5933 - acc: 0.7204\n",
      "Epoch 4/10\n",
      "4614/4614 [==============================] - 1s 279us/sample - loss: 0.5881 - acc: 0.7204\n",
      "Epoch 5/10\n",
      "4614/4614 [==============================] - 1s 265us/sample - loss: 0.5779 - acc: 0.7204\n",
      "Epoch 6/10\n",
      "4614/4614 [==============================] - 1s 278us/sample - loss: 0.5638 - acc: 0.7204\n",
      "Epoch 7/10\n",
      "4614/4614 [==============================] - 1s 250us/sample - loss: 0.5421 - acc: 0.7206\n",
      "Epoch 8/10\n",
      "4614/4614 [==============================] - 1s 245us/sample - loss: 0.4974 - acc: 0.7373\n",
      "Epoch 9/10\n",
      "4614/4614 [==============================] - 1s 262us/sample - loss: 0.4626 - acc: 0.7603\n",
      "Epoch 10/10\n",
      "4614/4614 [==============================] - 1s 246us/sample - loss: 0.4234 - acc: 0.7805\n",
      "2306/2306 [==============================] - 1s 570us/sample - loss: 1.1236 - acc: 0.2823\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 509us/sample - loss: 0.6543 - acc: 0.6610\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 210us/sample - loss: 0.6223 - acc: 0.6885\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 231us/sample - loss: 0.6174 - acc: 0.6885\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 237us/sample - loss: 0.6112 - acc: 0.6885\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 221us/sample - loss: 0.6046 - acc: 0.6885s - loss: 0.60\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 221us/sample - loss: 0.5988 - acc: 0.6917\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 252us/sample - loss: 0.5821 - acc: 0.7047\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 222us/sample - loss: 0.5742 - acc: 0.7065\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 216us/sample - loss: 0.5562 - acc: 0.7214\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 206us/sample - loss: 0.5393 - acc: 0.7470\n",
      "2307/2307 [==============================] - 1s 618us/sample - loss: 1.0098 - acc: 0.2124\n",
      "Train on 4613 samples\n",
      "Epoch 1/10\n",
      "4613/4613 [==============================] - 2s 492us/sample - loss: 0.6902 - acc: 0.5285\n",
      "Epoch 2/10\n",
      "4613/4613 [==============================] - 1s 206us/sample - loss: 0.6864 - acc: 0.5326\n",
      "Epoch 3/10\n",
      "4613/4613 [==============================] - 1s 198us/sample - loss: 0.6808 - acc: 0.5289\n",
      "Epoch 4/10\n",
      "4613/4613 [==============================] - 1s 214us/sample - loss: 0.6685 - acc: 0.5547\n",
      "Epoch 5/10\n",
      "4613/4613 [==============================] - 1s 230us/sample - loss: 0.6481 - acc: 0.6109\n",
      "Epoch 6/10\n",
      "4613/4613 [==============================] - 1s 229us/sample - loss: 0.6313 - acc: 0.6467\n",
      "Epoch 7/10\n",
      "4613/4613 [==============================] - 1s 256us/sample - loss: 0.6052 - acc: 0.6781\n",
      "Epoch 8/10\n",
      "4613/4613 [==============================] - 1s 254us/sample - loss: 0.5920 - acc: 0.6898\n",
      "Epoch 9/10\n",
      "4613/4613 [==============================] - 1s 227us/sample - loss: 0.5775 - acc: 0.6937\n",
      "Epoch 10/10\n",
      "4613/4613 [==============================] - 1s 228us/sample - loss: 0.5643 - acc: 0.6995\n",
      "2307/2307 [==============================] - 1s 607us/sample - loss: 0.6299 - acc: 0.6658\n",
      "Train on 4614 samples\n",
      "Epoch 1/10\n",
      "4614/4614 [==============================] - 3s 550us/sample - loss: 0.6307 - acc: 0.7104\n",
      "Epoch 2/10\n",
      "4614/4614 [==============================] - 1s 236us/sample - loss: 0.5936 - acc: 0.7204\n",
      "Epoch 3/10\n",
      "4614/4614 [==============================] - 1s 243us/sample - loss: 0.5909 - acc: 0.7204\n",
      "Epoch 4/10\n",
      "4614/4614 [==============================] - 1s 242us/sample - loss: 0.5872 - acc: 0.7204\n",
      "Epoch 5/10\n",
      "4614/4614 [==============================] - 1s 239us/sample - loss: 0.5829 - acc: 0.7204\n",
      "Epoch 6/10\n",
      "4614/4614 [==============================] - 1s 221us/sample - loss: 0.5787 - acc: 0.7204\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4614/4614 [==============================] - 1s 223us/sample - loss: 0.5701 - acc: 0.7208\n",
      "Epoch 8/10\n",
      "4614/4614 [==============================] - 1s 235us/sample - loss: 0.5626 - acc: 0.7202\n",
      "Epoch 9/10\n",
      "4614/4614 [==============================] - 1s 233us/sample - loss: 0.5559 - acc: 0.7254\n",
      "Epoch 10/10\n",
      "4614/4614 [==============================] - 1s 253us/sample - loss: 0.5467 - acc: 0.7384\n",
      "2306/2306 [==============================] - 1s 626us/sample - loss: 1.0022 - acc: 0.1418\n",
      "Train on 6920 samples\n",
      "Epoch 1/10\n",
      "6920/6920 [==============================] - 3s 443us/sample - loss: 0.6930 - acc: 0.5120\n",
      "Epoch 2/10\n",
      "6920/6920 [==============================] - 2s 255us/sample - loss: 0.6900 - acc: 0.5396\n",
      "Epoch 3/10\n",
      "6920/6920 [==============================] - 2s 257us/sample - loss: 0.6802 - acc: 0.5790\n",
      "Epoch 4/10\n",
      "6920/6920 [==============================] - 2s 239us/sample - loss: 0.6496 - acc: 0.6461\n",
      "Epoch 5/10\n",
      "6920/6920 [==============================] - 2s 264us/sample - loss: 0.6031 - acc: 0.6961\n",
      "Epoch 6/10\n",
      "6920/6920 [==============================] - 2s 267us/sample - loss: 0.5380 - acc: 0.7452\n",
      "Epoch 7/10\n",
      "6920/6920 [==============================] - 2s 272us/sample - loss: 0.4848 - acc: 0.7796\n",
      "Epoch 8/10\n",
      "6920/6920 [==============================] - 2s 257us/sample - loss: 0.4340 - acc: 0.8140\n",
      "Epoch 9/10\n",
      "6920/6920 [==============================] - 2s 265us/sample - loss: 0.3966 - acc: 0.8338\n",
      "Epoch 10/10\n",
      "6920/6920 [==============================] - 2s 248us/sample - loss: 0.3501 - acc: 0.8549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f75ef8eda10>,\n",
       "                   iid='warn', n_iter=5, n_jobs=None,\n",
       "                   param_distributions={'kernel_sizes': [[2, 2, 3], [2, 4, 4],\n",
       "                                                         [2, 5, 5], [2, 5, 4],\n",
       "                                                         [2, 4, 4], [2, 2, 2],\n",
       "                                                         [2, 5, 4], [2, 2, 5],\n",
       "                                                         [2, 4, 4], [2, 3, 5],\n",
       "                                                         [2, 5, 4], [2, 2, 3],\n",
       "                                                         [2, 4, 4], [2, 3, 3],\n",
       "                                                         [2, 5, 3]],\n",
       "                                        'num_filters': [[1, 1, 1], [2, 2, 2],\n",
       "                                                        [3, 3, 3], [4, 4, 4],\n",
       "                                                        [5, 5, 5], [6, 6, 6]]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_cnn_model, epochs=epochs) \n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "num_filters = []\n",
    "for i in range(1,7):\n",
    "    num_filters.append([i,i,i])\n",
    "\n",
    "kernel_sizes = []\n",
    "for i in range(1,16):\n",
    "    kernel_sizes.append([2, randint(2,5), randint(2,5)])\n",
    "\n",
    "param_dict = dict(num_filters=num_filters, \n",
    "                  kernel_sizes=kernel_sizes)\n",
    "\n",
    "\n",
    "n_iter_search = 5 # Number of parameter settings that are sampled.\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                             param_distributions=param_dict,\n",
    "                             n_iter=n_iter_search)\n",
    "grid_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.552168 using {'num_filters': [6, 6, 6], 'kernel_sizes': [2, 4, 4]}\n",
      "0.552168 (0.098488) with: {'num_filters': [6, 6, 6], 'kernel_sizes': [2, 4, 4]}\n",
      "0.286272 (0.229576) with: {'num_filters': [1, 1, 1], 'kernel_sizes': [2, 2, 3]}\n",
      "0.455491 (0.240878) with: {'num_filters': [3, 3, 3], 'kernel_sizes': [2, 3, 3]}\n",
      "0.523844 (0.182007) with: {'num_filters': [4, 4, 4], 'kernel_sizes': [2, 5, 5]}\n",
      "0.340029 (0.232175) with: {'num_filters': [2, 2, 2], 'kernel_sizes': [2, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
